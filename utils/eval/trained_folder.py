from functools import cached_property
import logging
import os
import os.path as osp
import tqdm

import torch
from torch.optim.swa_utils import update_bn
from torch_geometric.loader import DataLoader

from utils.DataPrepareUtils import my_pre_transform
from Networks.PhysDimeNet import PhysDimeNet
from Networks.UncertaintyLayers.swag import SWAG
from utils.LossFn import lossfn_factory
from utils.configs import Config, read_folder_config
from utils.train.trainer import data_provider_solver, remove_extra_keys
from utils.utils_functions import remove_handler, get_device, floating_type, init_model_test, solv_num_workers, validate_index


class TrainedFolder:
    """
    Load a trained folder for performance evaluation and other purposes.
    A trained folder is a folder generated by train.py. For example: 'exp_pl_005_run_2022-06-03_164656__676997'
    """
    def __init__(self, folder_name, config_folder=None):
        self.config_folder = config_folder
        self.folder_name = folder_name

        self._logger = None
        self._test_dir = None
        self._data_provider = None
        self._data_provider_test = None
        self._config_name = None
        self._loss_fn = None
        self._model = None

        self._ds_options = None
        self._ds_cls = None
        self._num_workers = None

        self.mae_fn = torch.nn.L1Loss(reduction='mean')
        self.mse_fn = torch.nn.MSELoss(reduction='mean')

    def update_bn(self):
        train_dl = DataLoader(self.ds[torch.as_tensor(self.ds.train_index)], batch_size=self.cfg["valid_batch_size"],
                             shuffle=False, num_workers=self.num_workers)
        update_bn(tqdm.tqdm(train_dl), self.model, device=get_device())
        torch.save(self.model.state_dict(), osp.join(self.folder_name, "best_model_bn_updated.pt"))

    def info(self, msg: str):
        if self.save_root is None:
            return
            
        self.logger.info(msg)

    @property
    def model(self):
        if self._model is None:
            model_data = torch.load(os.path.join(self.folder_name, 'best_model.pt'), map_location=get_device())
            net = init_model_test(self.cfg, model_data, None)
            self._model = net
        return self._model

    @property
    def num_workers(self):
        if self._num_workers is None:
            if not torch.cuda.is_available():
                # I use cpu to debug. Setting num_worker to 0 helps me to debug the data_loader
                self._num_workers = 0
                return self._num_workers

            n_cpu_avail, n_cpu, num_workers = solv_num_workers()
            if self.cfg.data.proc_in_gpu:
                num_workers = 0
                self.info("Since data will be preprocessed into GPU, num_workers will be set to 0")
            self.info(f"Number of total CPU: {n_cpu}")
            self.info(f"Number of available CPU: {n_cpu_avail}")
            self.info(f"Number of workers used: {num_workers}")
            self._num_workers = num_workers
        return self._num_workers

    @cached_property
    def loss_fn(self):
        return lossfn_factory(self.cfg)

    @cached_property
    def cfg(self) -> Config:
        cfg, config_name = read_folder_config(self.folder_name)
        self._config_name = config_name
        return cfg

    @property
    def ds(self):
        if self._data_provider is None:
            _data_provider = ds_from_args(self.cfg, rm_keys=False)

            # The lines below are dealing with the logic that I separate some test set from training set into
            # different files, which makes the code messy. It is not used in my relatively new datasets.
            if isinstance(_data_provider, tuple):
                _data_provider_test = _data_provider[1]
                _data_provider = _data_provider[0]
            else:
                _data_provider_test = _data_provider
            self._data_provider = _data_provider
            self._data_provider_test = _data_provider_test
        return self._data_provider

    @property
    def ds_test(self):
        if self._data_provider_test is None:
            # it was inited in self.data_provider
            __ = self.ds
        return self._data_provider_test

    @property
    def config_name(self):
        if self._config_name is None:
            __ = self.cfg
        return self._config_name

    @property
    def save_root(self):
        return None

    @property
    def logger(self):
        if self._logger is None:
            remove_handler()
            logging.basicConfig(filename=os.path.join(self.save_root, "test.log"),
                            format="%(asctime)s %(levelname)s %(message)s", force=True)
            logger = logging.getLogger()
            logger.setLevel(logging.DEBUG)
            self._logger = logger
        return self._logger

    @property
    def ds_options(self):
        if self._ds_options is None:
            self._ds_cls, self._ds_options = data_provider_solver(self.cfg, {})
        return self._ds_options

    @property
    def ds_cls(self):
        if self._ds_cls is None:
            __ = self.ds_options
        return self._ds_cls


def ds_from_args(cfg: Config, rm_keys=True, test_set=False):
    default_kwargs = {'data_root': cfg.data.data_root, 'pre_transform': my_pre_transform, 'record_long_range': True,
                      'type_3_body': 'B', 'cal_3body_term': True}
    dataset_cls, _kwargs = data_provider_solver(cfg, default_kwargs)
    dataset = dataset_cls(**_kwargs)
    if rm_keys:
        dataset = remove_extra_keys(dataset)
    print("used dataset: {}".format(dataset.processed_file_names))
    if dataset.train_index is not None:
        validate_index(dataset.train_index, dataset.val_index, dataset.test_index)
    return dataset
